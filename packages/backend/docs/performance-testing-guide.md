# Performance Testing and Optimization Guide

This guide covers performance testing, optimization, and cost management for the CICADA AgentCore implementation.

## Requirements

This implementation addresses the following requirements:
- **15.1**: Use cost-effective foundation models (Nova Lite/Micro)
- **15.2**: Optimize token usage through context management
- **15.3**: Configure appropriate resource limits
- **15.5**: Total Monthly Cost SHALL remain below $100

## Performance Testing

### Running Performance Tests

The performance test script measures:
- Agent invocation latency
- Time to first chunk (streaming responsiveness)
- Token usage per query
- Concurrent request handling
- Cost estimation

```bash
cd packages/backend

# Set environment variables
export ORCHESTRATOR_AGENT_ID=your-agent-id
export ORCHESTRATOR_AGENT_ALIAS_ID=your-alias-id
export AWS_REGION=us-east-1

# Run performance tests
pnpm run perf
```

### Test Coverage

The performance tests include queries that exercise different agent coordination patterns:

1. **Query Agent Only**: Simple script searches
   - "What does Rena say about the dam project?"
   - "Tell me about the Watanagashi Festival."

2. **Theory Agent**: Complex analysis requiring evidence gathering
   - "Analyze the theory that Mion and Shion are the same person."
   - "What evidence supports the time loop theory?"

3. **Profile Agent**: Knowledge extraction
   - "What do we know about Rika Furude?"
   - "Tell me about the Sonozaki family."

4. **Multi-Agent Coordination**: Complex queries requiring multiple agents
   - "Compare what Rena says about Oyashiro-sama in different episodes."
   - "How does the dam project relate to the curse?"

### Performance Metrics

The test reports the following metrics:

#### Latency Metrics
- **Average Latency**: Mean time to complete a query
- **Median Latency**: 50th percentile latency
- **P95 Latency**: 95th percentile latency (most queries complete within this time)
- **P99 Latency**: 99th percentile latency
- **Time to First Chunk**: How quickly streaming begins (user-perceived responsiveness)

#### Token Usage Metrics
- **Total Input Tokens**: Tokens sent to the model
- **Total Output Tokens**: Tokens generated by the model
- **Average Tokens per Query**: Mean token usage
- **Cost per 100 Queries**: Estimated cost for 100 queries
- **Estimated Monthly Cost**: Based on 100 queries/month requirement

#### Reliability Metrics
- **Success Rate**: Percentage of queries that completed successfully
- **Error Rate**: Percentage of queries that failed
- **Concurrent Performance**: How the system handles multiple simultaneous requests

### Budget Validation

The test automatically validates that costs remain within budget:

```
Monthly Budget: $100.00
Estimated Agent Cost: $0.03
Estimated Infrastructure Cost: $20.00
Total Estimated Cost: $20.03
Budget Utilization: 20.03%
✅ WITHIN BUDGET ($79.97 remaining)
```

## Optimization

### Running Optimization Analysis

The optimization script analyzes agent instructions and provides recommendations:

```bash
cd packages/backend
pnpm run optimize
```

### Optimization Strategies

#### 1. Concise Language
- Replace verbose phrases: "in order to" → "to"
- Remove filler words: "basically", "actually", "essentially"
- Use active voice instead of passive voice

**Example:**
```
❌ "In order to process the query, the agent should analyze..."
✅ "To process the query, analyze..."
```

#### 2. Eliminate Redundancy
- Remove duplicate instructions
- Consolidate similar bullet points
- Reference shared concepts once

**Example:**
```
❌ Multiple sections explaining error handling
✅ Single error handling section referenced by all agents
```

#### 3. Optimize Examples
- Limit to 2-3 most important examples per concept
- Use shorter, more focused examples
- Remove obvious examples

**Example:**
```
❌ 10 examples of citation formats
✅ 3 examples covering edge cases
```

#### 4. Simplify Structure
- Flatten nested bullet points
- Reduce markdown formatting overhead
- Use shorter section headers

**Example:**
```
❌ "## Detailed Instructions for Query Processing and Citation Generation"
✅ "## Query Processing"
```

#### 5. Focus on Essential Information
- Remove "nice to have" instructions
- Keep only critical error handling guidance
- Eliminate explanatory text that doesn't change behavior

### Token Reduction Impact

Token reductions directly impact cost:

```
Nova Lite Pricing:
- Input: $0.06 per 1M tokens
- Output: $0.24 per 1M tokens

Example Savings:
- Reduce instructions by 200 tokens per query
- 100 queries/month = 20,000 tokens saved
- Cost savings: (20,000 / 1,000,000) × $0.06 = $0.0012/month
```

While individual savings are small, they compound across:
- Multiple agents (4 agents)
- Multiple invocations per query (agent coordination)
- All queries over time

## Monitoring Performance in Production

### CloudWatch Metrics

The system emits the following custom metrics to CloudWatch:

```
Namespace: CICADA/Agents

Metrics:
- AgentInvocationCount: Number of invocations per agent
- AgentInvocationDuration: Latency per agent (milliseconds)
- AgentInvocationErrors: Error count per agent
- AgentTokenUsage: Total tokens per agent
- AgentInputTokens: Input tokens per agent
- AgentOutputTokens: Output tokens per agent
- AgentCoordinationLatency: Time for agent-to-agent calls
```

### Viewing Metrics

1. **CloudWatch Dashboard**: Navigate to the CICADA-Agents dashboard
2. **Custom Queries**: Use CloudWatch Insights to analyze logs
3. **Alarms**: Receive notifications for high latency or errors

### Example CloudWatch Insights Queries

**Average latency by agent:**
```
fields @timestamp, agentName, duration
| filter eventType = "agent_invocation_success"
| stats avg(duration) as avgLatency by agentName
| sort avgLatency desc
```

**Token usage by agent:**
```
fields @timestamp, agentName, tokenUsage.total
| filter eventType = "agent_invocation_success"
| stats sum(tokenUsage.total) as totalTokens by agentName
| sort totalTokens desc
```

**Error rate by agent:**
```
fields @timestamp, agentName
| filter eventType = "agent_invocation_failure"
| stats count() as errorCount by agentName
| sort errorCount desc
```

## Cost Optimization Best Practices

### 1. Model Selection

Use the most cost-effective model that meets requirements:
- **Nova Lite**: Best for most queries ($0.06/$0.24 per 1M tokens)
- **Nova Micro**: Even cheaper for simple tasks ($0.035/$0.14 per 1M tokens)
- **Avoid Nova Pro**: Only use if Nova Lite is insufficient

### 2. Context Management

Minimize context sent to models:
- Only include relevant episode context
- Limit conversation history to recent messages
- Compress profile information to essentials

### 3. Agent Coordination

Optimize agent invocation patterns:
- Only invoke agents when necessary
- Cache results when appropriate
- Avoid redundant agent calls

### 4. Streaming Optimization

Use streaming effectively:
- Start streaming immediately (better UX)
- Don't buffer entire response before streaming
- Handle partial responses gracefully

### 5. Resource Limits

Configure appropriate limits:
- Lambda timeout: 5 minutes (most queries complete in <30s)
- Lambda memory: 512MB (sufficient for agent coordination)
- DynamoDB: On-demand pricing (no wasted capacity)

## Performance Targets

Based on requirements and testing:

| Metric | Target | Rationale |
|--------|--------|-----------|
| Average Latency | < 10s | Acceptable for complex queries |
| P95 Latency | < 20s | Most queries complete quickly |
| Time to First Chunk | < 3s | Good perceived responsiveness |
| Success Rate | > 95% | High reliability |
| Cost per 100 Queries | < $0.10 | Well within budget |
| Monthly Cost | < $100 | Hard requirement |

## Troubleshooting Performance Issues

### High Latency

**Symptoms**: Queries taking longer than expected

**Possible Causes**:
1. Agent instructions too long → Run optimization analysis
2. Too much context being sent → Review context management
3. Multiple agent invocations → Check coordination patterns
4. Cold start issues → Consider provisioned concurrency

**Solutions**:
- Optimize agent instructions
- Reduce context size
- Cache frequently accessed data
- Monitor CloudWatch metrics for bottlenecks

### High Token Usage

**Symptoms**: Token usage exceeding estimates

**Possible Causes**:
1. Verbose agent instructions
2. Too much context in prompts
3. Long output generation
4. Redundant agent invocations

**Solutions**:
- Run optimization analysis
- Implement context compaction
- Add output length limits
- Review agent coordination logic

### High Error Rate

**Symptoms**: Frequent agent invocation failures

**Possible Causes**:
1. Throttling from Bedrock
2. Invalid agent configurations
3. Permission issues
4. Timeout issues

**Solutions**:
- Implement exponential backoff
- Review IAM permissions
- Increase Lambda timeout
- Check CloudWatch logs for details

## Continuous Optimization

### Regular Performance Reviews

Schedule regular performance reviews:
1. **Weekly**: Review CloudWatch metrics
2. **Monthly**: Run performance tests
3. **Quarterly**: Analyze cost trends and optimize

### Performance Testing in CI/CD

Consider adding performance tests to CI/CD:
```bash
# In CI/CD pipeline
pnpm run perf
# Fail if latency exceeds thresholds
# Fail if cost exceeds budget
```

### A/B Testing Optimizations

When making optimizations:
1. Test with a subset of queries
2. Compare metrics before/after
3. Validate that functionality is preserved
4. Roll out gradually

## Conclusion

Performance testing and optimization are ongoing processes. Use the tools provided to:
- Measure current performance
- Identify optimization opportunities
- Validate improvements
- Ensure costs remain within budget

Regular monitoring and optimization ensure the system remains fast, reliable, and cost-effective.
